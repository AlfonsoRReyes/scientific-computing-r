---
title: "Efficient accumulation in R"
output:
  html_document: default
  html_notebook: default
---

```{r setup, include=F, error=T, message=F, warning=F}
knitr::opts_chunk$set(echo=T, comment=NA, error=T, warning=F, message = F, fig.align = 'center')
```

Source: http://www.win-vector.com/blog/2015/07/efficient-accumulation-in-r/

R has a number of very good packages for manipulating and aggregating data (dplyr, sqldf, ScaleR, data.table, and more), but when it comes to accumulating results the beginning R user is often at sea. The R execution model is a bit exotic so many R users are very uncertain which methods of accumulating results are efficient and which are inefficient.

In this latest “R as it is” (again in collaboration with our friends at Revolution Analytics) we will quickly become expert at efficiently accumulating results in R.

A number of applications (most notably simulation) require the incremental accumulation of results prior to processing. For our example, suppose we want to collect rows of data one by one into a data frame. Take the mkRow function below as a simple example source that yields a row of data each time we call it.

```{r}
library(tidyverse)
```


```{r}
mkRow <- function(nCol) {
  rn <- rnorm(nCol)        # vector of `ncol` random numbers
  x <- as.list(rn)         # convert vector to a list
  # make row mixed types by changing first column to string
  x[[1]] <- ifelse(x[[1]] > 0, 'pos', 'neg')    # 1st element of list to be pos/neg
  names(x) <- paste('x', seq_len(nCol), sep = '.')   # name each member of the list
  x    # return the list/column
}
```


The obvious “for-loop” solution is to collect or accumulate many rows into a data frame by repeated application of rbind. This looks like the following function.
```{r}
# The common wrong-way to accumulate the rows of data into a single data frame.
mkFrameForLoop <- function(nRow, nCol) {
  d <- c()
  for(i in seq_len(nRow)) {
    ri <- mkRow(nCol)
    di <- data.frame(ri, stringsAsFactors=FALSE)
    d <- rbind(d, di)
  }
  d
}
```

This would be the solution most familiar to many non-R programmers. The problem is: in R the above code is incredibly slow.


## Testing `mkFrameForLoop`

```{r}
set.seed(23525) # make run more repeatable

nCol <- 10                            # number of columns
# timeSeq <- seq.int(100, 2000, 100)  # with this sequence last DF will have 2000 rows
timeSeq <- seq.int(10, 200, 10)       # with this sequence last DF will have 200  rows

timings <-  vector("list", length(timeSeq))

# timeSeq:   a vector indicating the number of rows for each iteration
# timings:   a list where the DF will be saved
```


```{r}
# This is the FOR loop that will create a list of many data frames
#
# timeseq:  is the number of rows for each data frame.
#           1st dataframe has 10 rows; 2nd 20; 3rd 30, and so on.           

for(i in seq_len(length(timeSeq))) {
  nRow <- timeSeq[[i]]                # get the size of the rows from timeSeq 
  # cat(i, nRow, nCol, "\n")
  ti <- mkFrameForLoop(nRow, nCol)    # traverse the rows
  
  ti <- data.frame(ti, stringsAsFactors = FALSE)  # convert to data frame
  ti$nRow <- nRow
  ti$nCol <- nCol
  timings[[i]] <- ti    # add data frame to a list
  
}  
# `ti` will contain the last data frame
# timings contains all the dataframes generated in the loop.
# 
cat("dataframes and dimensions\n")
sapply(timings, dim)
```


## Why the `for` loop takes so long?
In R all common objects are usually immutable and can not change. So when you write an assignment like `d <- rbind(d,di)` you are usually not actually adding a row to an existing data frame, but constructing a new data frame that has an additional row. This new data frame replaces your old data frame in your current execution environment (R execution environments are mutable, to implement such changes). This means to accumulate or add n rows incrementally to a data frame, as in mkFrameForLoop we actually build n different data frames of sizes 1,2,...,n. As we do work copying each row in each data frame (since in R data frame columns can potentially be shared, but not rows) we pay the cost of processing n*(n+1)/2 rows of data. So: no matter how expensive creating each row is, for large enough n the time wasted re-allocating rows (again and again) during the repeated rbinds eventually dominates the calculation time. For large enough n you are wasting most of your time in the repeated `rbind` steps.


...


Given this how does anyone ever get real work done in R? The answers are:

* Experienced R programmers avoid the for-loop structure seen in mkFrameForLoop.

* In some specialized situations (where value visibility is sufficiently limited) R can avoid a number of the unnecessary user specified calculations by actual in-place mutation (which means R can in some cases change things when nobody is looking, so only observable object semantics are truly immutable).


## Using `lapply`
The most elegant way to avoid the problem is to use R’s `lapply` (or list apply) function as shown below:

```{r}
# This function does not use FOR to iterate the rows
mkFrameList <- function(nRow, nCol) {
  d <- lapply(seq_len(nRow), function(i) { 
    ri <- mkRow(nCol)                         # make the row
    data.frame(ri, stringsAsFactors=FALSE)    # return a DF in the anonymous function
  })
  do.call(rbind, d)       # return a data frame
}
```


```{r}
# This is using `lapply` instead of `for` that will create a list of many data frames
#
# timeseq:  is the number of rows for each data frame.
#           1st dataframe has 10 rows; 2nd 20; 3rd 30, and so on.           

for(i in seq_len(length(timeSeq))) {
  nRow <- timeSeq[[i]]                # get the size of the rows from timeSeq 
  # cat(i, nRow, nCol, "\n")
  ti <- mkFrameList(nRow, nCol)    # traverse the rows
  
  ti <- data.frame(ti, stringsAsFactors = FALSE)
  ti$nRow <- nRow
  ti$nCol <- nCol
  timings[[i]] <- ti
  
}  
# `ti` will contain the last data frame
# timings contains all the dataframes generated in the loop.
sapply(timings, dim)
```

## Why `lapply` is faster?
What we did is take the contents of the for-loop body, and wrap them in a function. This function is then passed to `lapply` which creates a list of rows. We then batch apply `rbind` to these rows using `do.call`. It isn’t that the for-loop is slow (which many R users mistakingly believe), **it is the incremental collection of results into a data frame is slow** and that is one of the steps the `lapply` method is avoiding. While you can prefer lapply to for-loops always for stylistic reasons, it is important to understand when `lapply` is in fact quantitatively **better** than a for-loop (and to know when a for-loop is in fact acceptable). In fact a for-loop with a better binder such as data.table::rbindlist (assuming your code can work with a data.table which in some environments has different semantics) is among the fastest variations we have seen (as suggested by Arun Srinivasan in the comments below; another top contender are file based Split-Apply-Combine methods as suggested in comments by David Hood, ideas also seen in Map-Reduce).


## Collecting rows in a list: `mkFrameForList`
If you don’t want to learn about lapply you can write fast code by collecting the rows in a list as below.

```{r}
mkFrameForList <- function(nRow, nCol) {
  d <- as.list(seq_len(nRow))
  for (i in seq_len(nRow)) {
    ri <- mkRow(nCol)
    di <- data.frame(ri, stringsAsFactors=FALSE)
    d[[i]] <- di
  }
  do.call(rbind,d)
}
```

Testing it:
```{r}
for(i in seq_len(length(timeSeq))) {
  nRow <- timeSeq[[i]]                # get the size of the rows from timeSeq 
  ti <- mkFrameForList(nRow, nCol)    # traverse the rows
  ti <- data.frame(ti, stringsAsFactors = FALSE)
  ti$nRow <- nRow
  ti$nCol <- nCol
  timings[[i]] <- ti
  
}  
sapply(timings, dim)
```


The above code still uses a familiar `for-loop` notation and is in fact fast. Below is a comparison of the time (in MS) for each of the above algorithms to assemble data frames of various sizes. The quadratic cost of the first method is seen in the slight upward curvature of its smoothing line. Again, to make this method truly fast replace `do.call(rbind,d)` with `data.table::rbindlist(d)` ([examples here](http://winvector.github.io/Accumulation/Accum.html)).


The reason `mkFrameForList` is tolerable is in some situations R can avoid creating new objects and in fact manipulate data in place. In this case the list “d” is not in fact re-created each time we add an additional element, but in fact mutated or changed in place.

## In-place edits
(edit) The common advice is we should prefer in-place edits. We tried that, but it wasn’t until we (after getting feedback in our comments below) threw out the data frame class attribute that we got really fast code. The code and latest run are below (but definitely check out the comments following this article for the reasoning chain).

```{r}
mkFrameInPlace <- function(nRow, nCol, classHack=TRUE) {
  r1 <- mkRow(nCol)
  d <- data.frame(r1, stringsAsFactors=FALSE)
  if(nRow > 1) {
    d <- d[rep.int(1,nRow),]
    if(classHack) {
      # lose data.frame class for a while
      # changes what S3 methods implement
      # assignment.
      d <- as.list(d) 
    }
    for(i in seq.int(2,nRow,1)) {
      ri <- mkRow(nCol)
      for(j in seq_len(nCol)) {
        d[[j]][i] <- ri[[j]]
      }
    }
  }
  if(classHack) {
     d <- data.frame(d,stringsAsFactors=FALSE)
  }
  d
}
```

Testing it:
```{r}
for(i in seq_len(length(timeSeq))) {
  nRow <- timeSeq[[i]]                # get the size of the rows from timeSeq 
  ti <- mkFrameInPlace(nRow, nCol)    # traverse the rows
  ti <- data.frame(ti, stringsAsFactors = FALSE)
  ti$nRow <- nRow
  ti$nCol <- nCol
  timings[[i]] <- ti
  
}  
sapply(timings, dim)
```

Note that the in-place list of vectors method is faster than any of `lapply/do.call(rbind)`, `dplyr::bind_rows/replicate`, or `plyr::ldply`. This is despite having nested for-loops (one for rows, one for columns; though this is also why methods of this type can speed up even more if we use compile:cmpfun). At this point you should see: it isn’t the for-loops that are the problem, it is any sort of incremental allocation, re-allocation, and checking.


## Explaining  incremental allocation, re-allocation, and checking

At this point we are avoiding both the complexity waste (running an algorithm that takes time proportional to the square of the number of rows) and avoiding a lot of linear waste (re-allocation, type-checking, and name matching).

However, any in-place change (without which the above code would again be unacceptably slow) depends critically on the list value associated with “d” having very limited visibility. Even copying this value to another variable or passing it to another function can break the visibility heuristic and cause arbitrarily expensive object copying.

The fragility of the visibility heuristic is best illustrated with an even simpler example.

Consider the following code that returns a vector of the squares of the first n positive integers.

```{r}
computeSquares <- function(n,messUpVisibility) {
  # pre-allocate v
  # (doesn't actually help!)
  v <- 1:n
  if(messUpVisibility) {
     vLast <- v
  }
  # print details of v
  .Internal(inspect(v))
  for(i in 1:n) {
    v[[i]] <- i^2
    if(messUpVisibility) {
      vLast <- v
    }
    # print details of v
    .Internal(inspect(v))
  }
  v
}
```

Now of course part of the grace of R is we never would have to write such a function. We could do this very fast using vector notation such as seq_len(n)^2. But let us work with this notional example.

Below is the result of running computeSquares(5,FALSE). In particular look at the lines printed by the .Internal(inspect(v)) statements and at the first field of these lines (which is the address of the value “v” refers to).

```{r}
computeSquares(5,FALSE)
```

Notice that the address v refers to changes only once (when the value type changes from integer to real). After the one change the address remains constant (@7fdf0e2ba740) and the code runs fast as each pass of the for-loop alters a single position in the value referred to by v without any object copying.

Now look what happens if we re-run with `messUpVisibility`:

```{r}
computeSquares(5,TRUE)
```

Setting `messUpVisibility` causes the value referenced by “v” to also be referenced by a new variable named “vLast“. Evidently this small change is enough to break the visibility heuristic as we see the address of the value “v” refers to changes after each update. Meaning we have triggered a lot of expensive object copying. So we should consider the earlier for-loop code a bit fragile as small changes in object visibility can greatly change performance.

The thing to remember is: for the most part R objects are immutable. So code that appears to alter R objects is often actually simulating mutation by expensive copies. This is the concrete reason that functional transforms (like lapply) should be preferred to incremental or imperative appends.

R code for all examples in this article can be found [here](http://winvector.github.io/Accumulation/) (this includes methods like pre-reserving space, the original [vector experiments](http://winvector.github.io/Accumulation/benchmark.html) that originally indicated the object mutation effect), and a [library wrapping the efficient incremental collector](https://github.com/WinVector/daccum).
