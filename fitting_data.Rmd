---
title: "R Notebook"
output: html_notebook
---


## 
```{r}
# find the function that better fits the relation between these two variables.
x<-c(0.108,0.111,0.113,0.116,0.118,0.121,0.123,0.126,0.128,0.131,0.133,0.136)

y<-c(-6.908,-6.620,-5.681,-5.165,-4.690,-4.646,-3.979,-3.755,-3.564,-3.558,-3.272,-3.073)

```




http://stackoverflow.com/a/15045035
Here is an example of comparing five models. Due to the form of the first two models we are able to use lm to get good starting values. (Note that models using different transforms of y should not be compared so we should not use lm1 and lm2 as comparison models but only for starting values.) Now run an nls for each of the first two. After these two models we try polynomials of various degrees in x. Fortunately lm and nls use consistent AIC definitions (although its not necessarily true that other R model fitting functions have consistent AIC definitions) so we can just use lm for the polynomials. Finally we plot the data and fits of the first two models.

The lower the AIC the better so nls1 is best followed by lm3.2 following by nls2.

```{r}
lm1 <- lm(1/y ~ x)
nls1 <- nls(y ~ 1/(a + b*x), start = setNames(coef(lm1), c("a", "b")))
AIC(nls1)
```

```{r}
lm2 <- lm(1/y ~ log(x))
nls2 <- nls(y ~ 1/(a + b*log(x)), start = setNames(coef(lm2), c("a", "b")))
AIC(nls2)
```

```{r}
lm3.1 <- lm(y ~ x) 
AIC(lm3.1) # 13.43161
```

```{r}

lm3.2 <- lm(y ~ poly(x, 2))
AIC(lm3.2) # -1.525982
```

```{r}
lm3.3 <- lm(y ~ poly(x, 3))
AIC(lm3.3) # 0.1498972
```


```{r}
plot(y ~ x)
lines(fitted(nls1) ~ x, lty = 1) # solid line
lines(fitted(nls2) ~ x, lty = 2) # dashed line
```

ADDED a few more models and subsequently fixed them up and changed notation. Also to follow up on Ben Bolker's comment we can replace AIC everywhere above with AICc from the AICcmodavg package.



-----
http://stackoverflow.com/a/15044001
I would begin by an explantory plots, something like this :

```{r}
library(latticeExtra)
library(grid)

x<-c(0.108,0.111,0.113,0.116,0.118,0.121,0.123,0.126,0.128,0.131,0.133,0.136)
y<-c(-6.908,-6.620,-5.681,-5.165,-4.690,-4.646,-3.979,-3.755,-3.564,-3.558,-3.272,-3.073)

dat <- data.frame(y=y,x=x)

xyplot(y ~ x,data=dat,par.settings = ggplot2like(),
       panel = function(x,y,...){
         panel.xyplot(x,y,...)
       })+
  layer(panel.smoother(y ~ x, method = "lm"), style =1)+  ## linear
  layer(panel.smoother(y ~ poly(x, 3), method = "lm"), style = 2)+  ## cubic
  layer(panel.smoother(y ~ x, span = 0.9),style=3)  + ### loeess
  layer(panel.smoother(y ~ log(x), method = "lm"), style = 4)  ## log
```

```{r}
summary(lm(y~poly(x,3),data=dat))
```


```{r}
summary(lm(y ~ log(x), data=dat))
```


## Fitting a function in R
http://stackoverflow.com/questions/11844522/fitting-a-function-in-r


```{r}
library(ggplot2)

# Importing your data
mydata <- read.table(text='
    x   y
1   0 123
2   2 116
3   4 113
4  15 100
5  48  87
6  75  84
7 122  77', header=T)

x <- mydata$x
y <- mydata$y

qplot(x, y, data=mydata, geom="line")

```

```{r}
lm1 <- lm(1/y ~ x)
nls1 <- nls(y ~ 1/(a + b*x), start = setNames(coef(lm1), c("a", "b")))
AIC(nls1)

# this will give error
lm2 <- lm(1/y ~ log(x))
nls2 <- nls(y ~ 1/(a + b*log(x)), start = setNames(coef(lm2), c("a", "b")))
AIC(nls2)
```

```{r}
lm3.1 <- lm(y ~ x) 
AIC(lm3.1) # 13.43161

lm3.2 <- lm(y ~ poly(x, 2))
AIC(lm3.2) # -1.525982

lm3.3 <- lm(y ~ poly(x, 3))
AIC(lm3.3) # 0.1498972

```

```{r}
plot(y ~ x)
lines(fitted(nls1) ~ x, lty = 1) # solid line
lines(fitted(lm3.3) ~ x, lty = 2) # dashed line
```

```{r}
summary(lm(y ~ poly(x, 3), data = mydata))
```


## Fitting polynomial model to data in R
http://stackoverflow.com/questions/3822535/fitting-polynomial-model-to-data-in-r

```{r}
x <- c(32,64,96,118,126,144,152.5,158)  
y <- c(99.5,104.8,108.5,100,86,64,35.3,15)
```


To get a third order polynomial in x (x^3), you can do

```{r}
lm(y ~ x + I(x^2) + I(x^3))
```

http://stackoverflow.com/a/3824248
Which model is the "best fitting model" depends on what you mean by "best". R has tools to help, but you need to provide the definition for "best" to choose between them. Consider the following example data and code:

```{r}
x <- 1:10
y <- x + c(-0.5,0.5)

plot(x,y, xlim=c(0,11), ylim=c(-1,12))

fit1 <- lm( y~offset(x) -1 )
fit2 <- lm( y~x )
fit3 <- lm( y~poly(x,3) )
fit4 <- lm( y~poly(x,9) )

library(splines)
fit5 <- lm( y~ns(x, 3) )
fit6 <- lm( y~ns(x, 9) )

fit7 <- lm( y ~ x + cos(x*pi) )

xx <- seq(0,11, length.out=250)
lines(xx, predict(fit1, data.frame(x=xx)), col='blue')
lines(xx, predict(fit2, data.frame(x=xx)), col='green')
lines(xx, predict(fit3, data.frame(x=xx)), col='red')
lines(xx, predict(fit4, data.frame(x=xx)), col='purple')
lines(xx, predict(fit5, data.frame(x=xx)), col='orange')
lines(xx, predict(fit6, data.frame(x=xx)), col='grey')
lines(xx, predict(fit7, data.frame(x=xx)), col='black')
```
Which of those models is the best? arguments could be made for any of them (but I for one would not want to use the purple one for interpolation).

```{r}
AIC(fit1)
AIC(fit2)
AIC(fit3)
AIC(fit4)
AIC(fit5)
AIC(fit6)
AIC(fit7)
```



http://stackoverflow.com/a/3824080
Regarding the question 'can R help me find the best fitting model', there is probably a function to do this, assuming you can state the set of models to test, but this would be a good first approach for the set of n-1 degree polynomials:

```{r}
polyfit <- function(i) x <- AIC(lm(y ~ poly(x, i)))

as.integer(optimize(polyfit, interval = c(1,length(x)-1))$minimum)
```


http://stackoverflow.com/a/12428183
The easiest way to find the best fit in R is to code the model as:

```{r}
lm.1 <- lm(y ~ x + I(x^2) + I(x^3) + I(x^4))
lm.1
```

After using step down AIC regression
```{r}
lm.s <- step(lm.1)
```

## Curve fitting this data in R?
http://stackoverflow.com/questions/14645199/curve-fitting-this-data-in-r

For a few days I've been working on this problem and I'm stuck ...

I have performed a number of Monte Carlo simulations in R which gives an output y for each input x and there is clearly some simple relationship between x and y, so I want to identify the formula and its parameters. But I can't seem to get a good overall fit for both the 'Low x' and 'High x' series, e.g. using a logarithm like this:

```{r}
data <- read.table(text='
      x          y
1   0.2 -0.7031864
2   0.3 -1.0533648
3   0.4 -1.3019655
4   0.5 -1.4919278
5   0.6 -1.6369545
6   0.7 -1.7477481
7   0.8 -1.8497117
8   0.9 -1.9300209
9   1.0 -2.0036842
10  1.1 -2.0659970
11  1.2 -2.1224324
12  1.3 -2.1693986
13  1.4 -2.2162889
14  1.5 -2.2548485
15  1.6 -2.2953162
16  1.7 -2.3249750
17  1.8 -2.3570141
18  1.9 -2.3872684
19  2.0 -2.4133978
20  2.1 -2.4359624
21  2.2 -2.4597122
22  2.3 -2.4818787
23  2.4 -2.5019371
24  2.5 -2.5173966
25  2.6 -2.5378936
26  2.7 -2.5549524
27  2.8 -2.5677939
28  2.9 -2.5865958
29  3.0 -2.5952558
30  3.1 -2.6120607
31  3.2 -2.6216831
32  3.3 -2.6370452
33  3.4 -2.6474608
34  3.5 -2.6576862
35  3.6 -2.6655606
36  3.7 -2.6763866
37  3.8 -2.6881303
38  3.9 -2.6932310
39  4.0 -2.7073198
40  4.1 -2.7165035
41  4.2 -2.7204063
42  4.3 -2.7278532
43  4.4 -2.7321731
44  4.5 -2.7444773
45  4.6 -2.7490365
46  4.7 -2.7554178
47  4.8 -2.7611471
48  4.9 -2.7719188
49  5.0 -2.7739299
50  5.1 -2.7807113
51  5.2 -2.7870781
52  5.3 -2.7950429
53  5.4 -2.7975677
54  5.5 -2.7990999
55  5.6 -2.8095955
56  5.7 -2.8142453
57  5.8 -2.8162046
58  5.9 -2.8240594
59  6.0 -2.8272394
60  6.1 -2.8338866
61  6.2 -2.8382038
62  6.3 -2.8401935
63  6.4 -2.8444915
64  6.5 -2.8448382
65  6.6 -2.8512086
66  6.7 -2.8550240
67  6.8 -2.8592950
68  6.9 -2.8622220
69  7.0 -2.8660817
70  7.1 -2.8710430
71  7.2 -2.8736998
72  7.3 -2.8764701
73  7.4 -2.8818748
74  7.5 -2.8832696
75  7.6 -2.8833351
76  7.7 -2.8891867
77  7.8 -2.8926849
78  7.9 -2.8944987
79  8.0 -2.8996780
80  8.1 -2.9011012
81  8.2 -2.9053911
82  8.3 -2.9063661
83  8.4 -2.9092228
84  8.5 -2.9135426
85  8.6 -2.9101730
86  8.7 -2.9186316
87  8.8 -2.9199631
88  8.9 -2.9199856
89  9.0 -2.9239220
90  9.1 -2.9240167
91  9.2 -2.9284608
92  9.3 -2.9294951
93  9.4 -2.9310985
94  9.5 -2.9352370
95  9.6 -2.9403694
96  9.7 -2.9395336
97  9.8 -2.9404153
98  9.9 -2.9437564
99 10.0 -2.9452175', header = TRUE)

x <- data$x
y <- data$y

```

I have also tried to fit (log10(x), 10^y) instead, which gives a good fit but the reverse transformation doesn't fit (x, y) very well.

Can anyone solve this?

Please explain how you found the solution.



http://stackoverflow.com/a/14646118
Without an idea of the underlying process you may as well just fit a polynomial with as many components as you like. You don't seem to be testing a hypothesis (eg, gravitational strength is inverse-square related with distance) so you can fish all you like for functional forms, the data is unlikely to tell you which one is 'right'.

So if I read your data into a data frame with x and y components I can do:

```{r}
data$lx = log(data$x)
plot(data$lx, data$y) # needs at least a cubic polynomial 
m1 = lm(y~poly(lx,3), data=data) # fit a cubic
points(data$lx, fitted(m1), pch = 18, col = "red")
```

and the fitted points are pretty close. Change the polynomial degree from 3 to 7 and the points are identical. Does that mean that your Y values are really coming from a 7-degree polynomial of your X values? No. But you've got a curve that goes through the points.

At this scale, you may as well just join adjacent points up with a straight line, your plot is so smooth. But without underlying theory of why Y depends on X (like an inverse square law, or exponential growth, or something) all you are doing is joining the dots, and there are infinite ways of doing that.


## Fitting a curve to specific data
http://stackoverflow.com/questions/14190883/fitting-a-curve-to-specific-data
I have the following data in my thesis:

    28 45
    91 14
    102 11
    393 5
    4492 1.77
I need to fit a curve into this.



http://stackoverflow.com/a/15050715

Just in case R is an option, here's a sketch of two methods you might use.

First method: evaluate the goodness of fit of a set of candidate models This is probably the best way as it takes advantage of what you might already know or expect about the relationship between the variables.

```{r}
# read in the data
dat <- read.table(text= "x y 
28 45
91 14
102 11
393 5
4492 1.77", header = TRUE)

# quick visual inspection
plot(dat); lines(dat)
```

```{r}
 # a smattering of possible models... just made up on the spot
    # with more effort some better candidates should be added
# a smattering of possible models...
models <- list(
  lm(y~x, data = dat), 
  lm(y~I(1/x), data=dat),
  lm(y ~ log(x), data = dat),
  nls(y ~ I(1/x*a) + b*x, data = dat, start = list(a = 1, b = 1)), 
  nls(y ~ (a + b*log(x)), data=dat, 
      start = setNames(coef(lm(y ~ log(x), data=dat)), c("a", "b"))),
  nls(y ~ I(exp(1)^(a + b * x)), data=dat, start = list(a=0,b=0)),
  nls(y ~ I(1/x*a)+b, data=dat, start = list(a=1,b=1))
)

# have a quick look at the visual fit of these models
library(ggplot2)
    ggplot(dat, aes(x, y)) + geom_point(size = 5) +
  stat_smooth(method = "lm", formula = as.formula(models[[1]]), size = 1, se = FALSE, colour = "black") + 
  stat_smooth(method = "lm", formula = as.formula(models[[2]]), size = 1, se = FALSE, colour = "blue") + 
  stat_smooth(method = "lm", formula = as.formula(models[[3]]), size = 1, se = FALSE, colour = "yellow") + 
  stat_smooth(method = "nls", formula = as.formula(models[[4]]), data=dat, start = list(a=0,b=0), size = 1, se = FALSE, colour = "red") + 
  stat_smooth(method = "nls", formula = as.formula(models[[5]]), data=dat, start = setNames(coef(lm(y ~ log(x), data=dat)), c("a", "b")), size = 1, se = FALSE, colour = "green") +
  stat_smooth(method = "nls", formula = as.formula(models[[6]]), data=dat, start = list(a=0,b=0), size = 1, se = FALSE, colour = "violet") +
  stat_smooth(method = "nls", formula = as.formula(models[[7]]), data=dat, start = list(a=0,b=0), size = 1, se = FALSE, colour = "orange")
```

The orange curve looks pretty good. Let's see how it ranks when we measure the relative goodness of fit of these models are...
```{r}
# calculate the AIC and AICc (for small samples) for each 
# model to see which one is best, ie has the lowest AIC
library(AICcmodavg); 
library(plyr); 
library(stringr)

ldply(models, function(mod){ data.frame(AICc = AICc(mod), AIC = AIC(mod), model = deparse(formula(mod))) })

# y ~ I(1/x * a) + b * x is the best model of those tried here for this curve
# it fits nicely on the plot and has the best goodness of fit statistic
# no doubt with a better understanding of nls and the data a better fitting
# function could be found. Perhaps the optimisation method here might be
# useful also: http://stats.stackexchange.com/a/21098/7744
```

